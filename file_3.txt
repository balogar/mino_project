The aim of this Bachelor thesis is to introduce the basic concepts of regression analysis and subsequently regression splines as parametric models for regression function. I have looked upon the main characteristics of regression splines (coherence, coherence of derivations, the choice of placement and a number of knots). Further on in the thesis I have studied two bases as the examples of regression splines (truncated power basis and B-spline basis). I have also presented a model of natural cubic splines and a suitable basis for its representation has been derived. In the other part of my thesis I have looked upon the use of natural splines in order to increases the appraisal precision of regression function, mean square error formula has been derived and I have been trying to find out and illustrate under what conditions the use of natural splines is applicable. The thesis is complemented with a Monte Carlo Simulation, contextualized into models of splines. The results show that the criteria commonly used for the choice of a model (R2adj, PRESS statistic, hypothesis testing) do not always enable us to choose the right model in order to achieve the greatest precision of the estimation of regression function. All the calculations are done in R software and are in the electronic attachment. Introduction Many problems, already in technical or economic sectors, include examining the relationship, the dependence or independence between two or more. Regression analysis deals with the solution of these problems. Regression analysis is an important statistical tool that includes a number of methods by which we estimate, among other things, the mean value of some worth. We call this random variable a response or dependence on the. Its average value is estimated on the basis of one or more companies discrete quantities called regressors, whether or not variables. If we examine the dependence of the mean value of the dependent variable on the notes of one independent variable, we are talking about the so-called simple linear regression, whereas when examining the dependence on the values several independent variables, about multiple regression analysis. The dependence between the mean value of the dependent and the given independent variables will describe a function we call a regression function. Parameter curve The metric prescription of the regression function can take a number of different forms, from which we will primarily focus on the regression function of splines. In the introduction to Chapter 2, let us imagine one of the simplest regression parts - regression model of a line. It is clear from the name that the relationship describing dependence the mean value of the dependent and independent variable will be linear regressive the function in this case will be a (regression) line. It is on this model that you are we show as from the data that form a set of observations of the mentioned variables, we calculate estimates of regression coefficients determining the shape of the estimated regression straight line. We calculate the estimates using the least squares method, which is closer we will describe. Next, in, we define a linear model of multiple personal regression. Next, we will define a submodel and show it two approaches as we can get from the model. You will further define we take an F-test to test the possible validity of the submodel. is devoted to various criteria for the selection of the regression model. we introduce the regression splines. We will describe their basic properties and then let imagine two different systems of basis functions, which are linear by the combination we get the regression function of the regression spline. Next, we will discuss the various approaches to position selection and the number of nodes. Keep going we present a model of natural splines and derive a suitable basis for its presentation. we will discuss under what circumstances it is appropriate to use natural splines in order to increase the accuracy of the regression function estimate. At the end We will work with Monte Carlo to simulate the quality of research, based on The practice of commonly used criteria for the selection of the regression model can be decide whether the model is, in terms of the accuracy of the regression function estimate, optimal. Linear regression model We can understand the model of linear regression as a prescription, which we will describe in more detail. relationship between the independent and independent variables. Definition of a multiple model linear regression can be found in , but for clarity and better understanding procedures and methods for investigating the relationship between the mean value of the coincidence quantity and the values one non-random quantity, we will show on the example regression model of a line. In this context, we will assume that we are independent variable is a non-random variable that we will denote by. As we have already indicated at the beginning of the chapter, we assume that the regression function describing the relationship between the variables will be a straight line. We will compile our data observed where Yi represents the i-th value of the dependent and will in turn be the i-th value of the independent variable. Regression The line model can be written in the form which can be interpreted in such a way that for the given constants they consist leading values Yi from the value of and some random deviation marked i. In order for this model to meet the assumptions of a linear regression part, must apply, inter alia, for each Then from the equation fill in Equation gives us the value of the regression function in the i-th observation. We don't know parameters, called regression coefficients, represent the intersection of a line with the and the direction of our regression line. We will try these parameters from our data is best estimated. Estimates of the coefficients , or the mean value of the response Yi, will be marked with the symbol of the roof, or Yi. An estimate of Yi, which we call the i-tou balanced value, we can write ’in the form Estimates will be searched using the least squares method, in more detail described in. With this method we will minimize the sum of the squares of the error, which we call the SS (from the English sum of squares), determined by the regulation the expression is a function in the unknown parameters. As we are already indicated, we want to find such values parameter, in which it is minimum, it will be a problem of finding the minimum of a function of two variables. The values which  is the minimum are the estimates of the least Squares. Subsequently, we place the derivative equal to zero, thus obtaining a system of equations, which we call the system of normal equations. After dividing these by equal numbers (−2) and the divorce sum will get our searched estimates if System equ  has exactly one solution if at least two of the values are different. It is obvious that the minimized function is convex, so the solution is truly a global minimum. By solving equations we get estimates in the form where  1are arithmetic means. In practice, however, this procedure calculates regression coefficients, does not use. A more suitable alternative is to use lm () function in the software R. Further in the work we will calculate the estimates of the right using this function. The regression model of the line may not be flexible enough in some cases to describe sufficiently well the relationship between the mean value of In practice, therefore, it is common to describe this relationship in more flexible for example quadratic or cubic polynomials, or regression splines, which are special cases of a linear model of multiple regression. You are we will define in the next section 2.2. The above-mentioned special cases functions (and many more) will then be discussed. Multiple linear regression model The multiple linear regression model differs from the regression line model assuming that for each value of the random variable is observed in general to the various values of known non-random constants independent variables that in some way affect the secondary value of quantities Y1, ..., Yn. Assume that the relationship between the mean value of and the given values of inconvenient constants can be described as follows  we do not know the parameters, regression coefficients. Known the constants occurring in equation (2.9) is suitable for their further use of the arrangement data into a matrix, which we will denote by and we will call a regression matrix or a matrix model, Note. In this work we will assume that the columns of the regression matrix are linear independent, so the matrix will have the full rank . The first column of the regression matrix, corresponding to the unit vector, corresponds to to the regression parameter , which we call the absolute term. This member tells us the expected value of the quantity in the case if all the explanatory variables were same zero. We will continue to consider the notion of a linear regression model. (see Def.) we mean a linear regression model with an absolute term. The concept linear stresses that the relationship describing the dependence between dependent and independent variables are linear in regression coefficients. Note. The constants do not have to contain only the observed values, but they can take the form of transformed original comparisons, e.g. transformation one independent variable  in the form  in Chapter 3 we get to the prescription of the quadratic regression model. Transformed constants ij is used to call regressors. The following definition of a linear regression model is formulated according to [3]. Definition . Let is a matrix of known real constants determined by regulation (2.10). . We say that together with the matrix of model satisfies the linear regression model, if for the vector of unknown regression parameters which we call error terms, are components of a random vector, for which E , where  is another unknown parameter. From Definition  we can deduce that the random vector has a mean value  is a variation matrix , by which we assume the same variance and of the individual components of the random vector Linear (regression) mode we will write in the form   or we can write it’vector as  If we specially consider that the vector has multidimensional normal distribution, then the model  will be called line normal linear ’ model. In this model,  holds. Considering the given assumption in section 2.3 we will show the basic properties and distribution of statistics related to (not only) the estimation of the vector of regression coefficients, which we obtain by the method smaller squares. 2.3 Least squares method The least squares method  is one of the most widely used methods in the calculation of regression coefficients. Let us denote the vector of estimated re- coefficients and as in section, the values vector , which we call the vector of balanced values, will be the estimated mean values of the dependent variable. The individual components of the vector are as follows Relation can be written as, where the vector is the i-th row of the matrix. Before we show the calculation of the vector , we give the definition formulated according to , which it is closely related to the least squares method. Definition 2. The random vector will be called the residual vector and its individual components determining the distance between Yi and iYi,  we will call it a residual. Next, a great deal we will call the residual sum of squares. We will look for the vector  so that the RSS the distance between the vectors to, is as small as possible in Euclidean space. According to is we set equal to zero, which gives the system linear equations. Vector by solving the system of (normal) equations The minimized function (2.13) is convex when its second derivative according to the age is equal to, which is a positive semidefinite matrix (see Hardness ). The system solution (2.14) will therefore be a global minimum. Definition 3. The symmetric matrix A is positive definite if for each vector  The square matrix A is symmetric if Proposition The matrix is positive definite. so the matrix is symmetric. Consider some vector  Expression is the sum of squares, so it is indestructible. From the linear independence of the columns of the matrix follows that, so expression 2.15 is positive. Assuming a linear independence of the columns of the matrix , the square matrix is a regular, and therefore there is exactly one solution of a given system (2.14), and to The vector can be written in the form, taking the matrix We denote by so . Note. The designation of the matrix  by the letter H is used in regression. best analysis according to the English word hat, roof. Proposition 2. The matrix H is idempotent  and valid . Proof. It is obviously substituted by . Properties of estimates obtained by  In the following sentences we will show the basic properties of the vector estimation coefficients obtained by the least squares method, as well as an estimate of response mean vector. For our further needs, we introduce the matrix  and its we denote the individual components by vij, Theorem Corollary. Using the least squares method, we obtained unbiased vector estimates . Considering the normal linear model, we can  to formulate further sentences by means of which we determine the divisions of sensitive statistics. Now, in the next section 2.4, we will define another important concept of linear regression, namely a submodel. Next, we show two ways to obtain the submodel, a by omitting the columns of the regression matrix and introducing linear constraints on the regression coefficient of the linear regression model. In practice, it is often required that the model describing the relationship between currency and regressors was the simplest. So we are looking for a model that reduces the space (see Note below) of all possible middle value of the random vector. We call this model a submodel. Let take his definition formulated in accordance with. Note. We will use the symbol  to denote the linear envelope of the columns of the matrix. This linear space, which is formed by all linear combinations of columns, cov matrix, we call the regression space. It is obvious that, though. Comparison of model and submodel is one of the most important methods in regression analysis. As we can see in sentence 6, model comparison is based on on the differences between the residual squares in the model that we will mark ’ By default (according to Def. 2) RSS and the residual number of squares in the submodel, which we will mark RSS0. To derive the shape of the test statistics on the transition to consider the submodel again of the normal linear mode Comparison of model and submodel is one of the most important methods in regression analysis. As we can see in sentence 6, model comparison is based on on the differences between the residual squares in the model that we will mark ’ By default (according to Def. 2) RSS and the residual number of squares in the submodel, which we will mark RSS0. To derive the shape of the test statistics on the transition to submodel, let consider again a normal linear model. Statistics F0 from sentence 6 is a test statistic of the so-called F-test, which is zero hypothesis H0 is not rejected in favor of the alternative, which means that the conditions In this work, we will show two approaches to obtaining a submodel, by deleting columns of the matrix and by introducing linear constraints on the vector. Types of The following methods of obtaining the submodel will be used in Chapter 3. e.g. in obtaining the regression spline submodel, namely the model of natural spline nov, The first, simpler way to move from model to submodel is to delete wall of the regression matrix. Assume the decomposition of the matrix  model  and the corresponding decomposition of the vector  We can write our model in the form is denoted by RSS0 residual sum of squares in submodel , we can and hypothesis H0: test the transition from model (2.20) to the mentioned submodel Linear constraints on regression coefficients The second way we can get the submodel is to introduce linear limits. depending on the vector of regression coefficients. Linear constraints can be expressed in the form , where the matrix T has the dimensions and the vector the constant c has dimensions. note. The value of determines the number of constraints applied to the regression coefficients Let us now consider the theorem formulated according to [] that the given linear They really determine the submodel. We will again search for the estimate of the mean value of using the least squares method, thus by minimizing holds at the same time. The vector , which represents the estimate of EY in the submodel, will be found so that the distance ’between vectors 0Y0 and was as small as possible in the space. Vector the vector of estimated regression coefficients in the submodel, which, we obtain using the method of Langrage multipliers, taking into account we consider the minimized function in the form where the components of the vector are multipliers. The procedure for solving this The minimizing problem can be found in], from where it is taken following the vector shape  and boiling  as follows At the end of this chapter, we'll talk about predictions. This topic creates basis of regression analysis, when we often want to find out the prediction of individual or estimate of the mean value of an independent variable in a new one, which we have not yet observed point. This topic as well In section 2.5 we define the form of the individual forecast and the mean estimate values of the dependent variable as well as indicators of how well the data is translated estimated regression function of the linear model. An estimate of the mean value of the independent variable from the ground The concept of forecasting often comes up with estimates of future value depending on the regressor values we have already observed. IN In the context of linear regression, an estimated value of a random variable is predicted. at some new point considering and regressors  are again inconvenient constants. The following theory follows relies on the interpretation of the book [5, Sec. 3.6.5]. Consider the model. Predicting individual value depends we consider the variable at point in the form where error nez is independent of the components of the linear regression model vector. Assume that. For the (theoretical) mean value of  then holds We return to the predictions in subsection 2.5.3, where the theory described above we use e.g. for the definition of the so-called PRESS statistics, and then later in the section 3.7. Before we get to the mentioned subsection, let define a few other terms needed to understand and derive terms in a given subsection. In this subsection, we will define the additional sums of squares often used in regression analysis and we introduce a sentence that describes the relationship between them and RSS. Recall that in the work we consider a linear (regression) model, which (usually) contains an absolute member. This assumption corresponds to the fact that some of the columns of the regression matrix is identically equal to 1n. According to then it meets a weaker requirement The term SST defined by regulation (2.27) can be understood as an overall variation litu values depend on a variable. The term SSR determined (2.28) represents a part of visibility, which we can explain by the regressors, the so-called explained variability, while the RSS value indicates the so-called unexplained variability. Using Definition 6 and Sentence 9, as well as the theory described in subsection, We can move on to the definition and definition of expressions that express to us how well Part describes the relationship between the dependent variable and the regressors. In the following section we will show the prescription of the coefficient of determination together with its adjusted form. and then we use the method of multiple cross-validation to define so-called PRESS STATISTICS. We will often use these indicators in the next chapter 3 to use. Among the most common indicators of how well the model describes the depending on the variable and regressors in the model, the coefficient determination and adjusted coefficient of determination. Their values can be read from output of the summary () function (applied to the output of the lm () function in the R software), as R-squared and Adjusted R-squared values. The value of R2 tells us about the percentage explanation of the variability of the values Yi is our model and provides an indication of how well we can predict values on the basis of the regressors contained in the model ([8]). This greatness acquires a value from the interval , while the value  acquires at perfect translation of data by estimating the regression function, if for each. Under this assumption, . Conversely, the coefficient R2 acquires the value 0 if the model contains only an absolute member (see Lemma 10 below) none of the regressors gives us any information about the the mean value of the dependent variable. It is not easy to think that R2 is a non-decreasing function of the number of regressors. This follows from the fact that SST is independent of the number of regressors, while RSS adding a regressor (or regressors) to the model will reduce (or remain unchanged). FROM This feature implies that when comparing the model and its submodel with the independent variable, it is not appropriate to use this coefficient as an indicator better approximation of the data by the estimated regression function of the model. For comparison such models are more appropriate to use, e.g. just modified version of the coefficient R2adj, which takes into account the number of regressors (which is equal to provided in the model and is defined by the rule (2.31). Another quantity describing the accuracy of the regression model is the value PRESS (English predicted residual sum of squares). We obtain this by the  cross method validation so that we gradually omit the i-th observation from our data and the remaining observations can be used to calculate the estimation of the vector of regression coefficients in deli. Estimation of the vector of regression coefficients in the model with the omitted denoting and the mean value of the dependent variable in the omitted we estimate the observed as  Note. The estimate of is always calculated with one omitted observation, and therefore, the given procedure for obtaining the value of equation (2.35) is sometimes called  (English). Leave-one-out cross validation). In order to avoid multiple estimation of regression coefficients in the model, we use the validity of equality (2.34) (see [13, . 45]) deciding which of the (different) models describes the relationship between the variables we better choose the one whose value (2.35) will be lower. In this chapter, we have introduced the basic concepts of linear regression. we developed a linear model of multiple regression and demonstrated how estimates regression coefficients are calculated by the least squares method. We are next they have shown the basic properties of these estimates and their properties if we consider multidimensional normal distribution of the random vector. Next, we they renamed the concept of the submodel and the sentence about the transition to the submodel, which we will follow in the following chapter 3 odkaz refer to ’. At the end of Chapter (2), we defined several indicators determining how well the model describes the dependence between dependent and independent variables. We will now look at some specific regulations in the next chapter various linear models, the gradual combination of which we get to regulation of the regression spline model and how to consider linear constraints discussed in section 2.4 we get to their submodel - the model of natural resources. new Moon Splines The origin of the word spline can be found in naval engineering, where the term denoted a tool for drawing curves. Today we meet the word spline outside also in regression analysis, where we call the spline a polynomial function. The points at which individual polynomials are called are called nodes and their We will explain the meaning in section 3.2. Before we get to the prescription of the model regression spline, we transfer the numerator to the model of regression polynomials (section), as well as the model of the jump regression function (section 3.2), which, as we will see, they are closely related to the regression spline model. In Chapter 2, we defined a linear model of multiple regression in which we considered that together with the values of the dependent variable are generally observed various regressors. However, we will continue to work only on the models in which together with the random variable one continuous (non-random) variable is observed. As in the previous chapter, it will be considered that our data consists of observations of given quantities. Regression function like this models will be considered in the form where we know, linear independent functions. These features we will call the base functions. Definition 8. The set of linearly independent known functions we will call it a system of basis functions. Note. The basic function corresponding to the absolute member  will be mathematically (except for some exceptions) consider as f0. The individual examples of models presented in this chapter are obtained by considering various rules of basis functions Obviously, we are re- the gross model of the line () is expressed by a linear combination of basis functions so  is the base of the vector space of all mok. By prescribing the basis functions fj and we can generate a base for j regression model of a polynomial of any degree in the form, which we will imagine straight away in the following section. Polynomial regression model As we mentioned in Chapter 2, the regression line model does not have to, and v In practice, due to its simplicity, it is often not a very suitable model for write a relationship of dependence between the dependent and independent variable. Especially if there is a relationship describing the av dependence ’between variable nonlinear, the model of the line is directly usable. As the name of the section suggests, by considering more complex functions, such as polynomials, we can translate the data by estimating the regression function of the model will significantly improve. Consider that the regressors in the linear model of multiple regression (2.11) We pay for the transformation of our one independent variable, namely of the higher order. Our model takes shape for which preserves the properties of the general model (2.11) and we call it regression polynomial model (one independent variable). In this model we can use the statistics from Theorem 6 and the hypothesis to test, it would not be appropriate to go to the model submodel of regression polynome of degree, namely to the model of regression polynome of degree . Model (3.2) is still linear in parameters, since its regression function is expressed by a linear combination of basis functions, although the function describing the relationship the mean value of the independent variable and the independent variable is not linear (for by choosing). In practice, the degree of polynomial is chosen to be a maximum of 3 or 4, respectively the curve of higher polynomials can be too flexible and acquire too oscillating Face. It is obvious that when choosing we get the shape of our regression model line (relation ) and by choosing we get quadratic (resp. cubic) depending on the mean value of on the explanatory variable. As we see below in Figure, by increasing the degree of the polynomial we get better estimates the mean value of the response, in terms of increasing the value of R2adj for the choice Note. The data in Figure come from the MASS package in the R. Hod- The notes of the dependent variable Yi will be the observations of accel (acceleration) and value independent variable  observation times (cas) of dataset cycle. Let just think observations for which the value of the times variable is greater than 19. The size of this data file is. We will work with these data in Jump regression model The main idea of the jump function model is to divide the interval into measured values of the independent variable [min (), max ()] for several different non-overlapping sections. Points that divide us by a range of independent values variable to smaller subintervals, denote min . The points are called internal nodes (or only nodes or nodes). points) and points are called boundary nodes - they determine the boundaries of the interval translating data by estimating the regression function. Suppose these points are predefined constants. Data  translated by estimates of polynomials of various degrees. Adjusted coefficients of determination of individual (rounded to 2 decimal places) places) of election for Node points divide us by a range of measured values of the independent variable on on which we will specialize the data first by estimating the constant and then a linear function. To show the first type of approximation, estimate constants, we define the indicator function  as follows Acquires the jump function model with nodes or  in the model  The individual regression coefficients represent the difference between the mean value of the dependent variable Yi on the subinterval I0 and the interval Ik. From the prescription EYi (at individual intervals) we can see that EYi is after In parts, a constant function  with jumps in nodes. In Figure 3.2 we can see our data translated by estimating the regression function model (3.4) with two nodes chosen equally on the interval. Model in parts of a linear function We will now move on to a slightly more complex model, namely data approximation by estimating the parts of the linear function. We will give individual subintervals (3.3) The data is translated by estimating the regression function of the jump function model, thus, a constant on individual subintervals separated by a vertical intermittent A line showing the position of nodes. translate line estimates. It is obvious that the given model will contain twice more regression coefficients than the model (3.4). We define the model by parts of linear functions with nodes by a rule individual intervals, the regression function is a straight line. Hypothesis we can test the transition to the model submodel (3.5) - the jump function model (3.4). In the graph in Figure 3.3 we see the translation of the data by estimating the line on one subintervals with two nodes. By adding other power transformations of the independent variable, we would they could extend the model (3.5) so that we could translate the individual sections by estimation polynomial of degree. Before we do so, we modify our model (3.5), so that the individual lines at the nodal points are stacked on top of each other, that it is our regression function in join nodes. Now, according to [14], we will show an intuitive way of obtaining the model in linear regression function. Without prejudice to generality, let consider some arbitrary ones node. Let us further assume that this point is independent of our values divides the variable into two subintervals, on which we will translate the data snake lines. On the interval let us consider a line determined by the rule Translated by estimates of parts of linear functions separated by vertical dashed lines showing the position of the nodes represents the change the direction of the line at the transition from the interval  to the interval. Equation we can therefore rewrite the form  where do we get The value of the regression function in the i-th observation of our considered model the model with one nodal point, in which it is regression function of the conjunction, we can express in the form function has continuous derivations, derivation at point jump about size !. Model of parts of a continuous linear regression function with (internal) nodes, is defined by the generalization (3.7) as In the graph in Figure we can see the translated data by estimating the regression Model functions 3.9. As can be seen from Figure, the regression function estimate model (3.9) (with two nodes) acquires sharp edges in nodes. There is no differentiable function in the given nodes. We can solve this problem to solve with the help of a base generated by higher powers of the truncated power function. kcie. The basis of the function of a connected polynomial in nodes of degree consists of functions linear combination of given basis functions we get to the rewriting and definition of the regression spline. General spline model As we have shown by the successive continuity of the polynomial regression model and the model of the jump regression function, splines can be imagined as parts combine polynomial functions Splines with predetermined nodal points will be called regression splines. A regression spline model of degree with internal nodal points can be expressed in the form By choosing we get (in corresponding order) model of linear (see (3.9)), quadratic and cubic spline. The cubic spline model is one of the most widely used models. regression splines. Since it connects the first and second derivatives, the curve determining The cubic spline is smooth enough for most practical problems. To the model cubic spline, we return to section 3.6, where we consider the restriction on the regression function of this model we get the prescription of its submodel, namely the natural model splines. In Figure 3.5 we can see the system of basis functions of the regression model spline (3.10) with option and option in Figure 3.6 with two nodes points. from the rule (3.10) it is clear that by considering we would became the rule (3.2) of the regression model of polynomial grade, the polynomial model is a submodel of the regression spline model. Considering in mo- part (3.10) we could use the test statistics introduced in Theorem 6 and a hypothesis examine the validity of the submodel, the relationship between the variables, it was not more appropriate to describe the model of regression polynome. Note. In the electronic attachment (file functions.R) we can find our the programmed function rs (), which we used in the construction of Figure 3.7.  the name function generates a regression matrix of the model (3.10). Her arguments are, do of which we enter the values of an independent variable, then the argument of the nodes to which we specify the sequence of (internal) nodes and the degree argument, which we enter degree of spline. From Figure 3.7 below, we can see how the degree of spline gradually increases. we get (within the framework of our data file) an ever better estimate of the regression function, in terms of increasing the value of R2adj with increasing degree. Intermittent The nodes represent the nodes selected evenly on the interval [min , max ]. the adjusted coefficients of determination (rounded to 2 decimal places) are B-spline To estimate the regression coefficients of the spline model, it is possible to use in the software R built-in bs () function from the splines package. This function generates a regression for us matrix with other basis functions, such as the above basis functions (3.11) regression spline model. The basic functions of this model do not have to be suitable when, for nodes located immediately close to each other, these functions are almost linearly dependent on what we read in ([13, . 70]), at high the number of nodes can lead to a number of estimates of regression coefficients to the numerical stability. Therefore, in practice equivalent bases are used (bases generating the same space), which is used to calculate estimates of regression coefficients ricky more stable. B-spline base is one of the most used. How can we in [13. 70], balanced values of of the regression splines nov with a base (3.11) and a base formed by B-spline basis functions, for both base words. Before we define the rule of B-spline basis functions, we must our sequence of internal and border nodes to expand. Consider the next 2 ( external nodes, which are necessary for the construction of B-spline basis functions series  Let us denote the increasing sequence of nodes. Then B-spline basis function of series   with a sequence of nodes, given by the recursive regulation according to [9, . 90] as follows With this recursion rule we can calculate the B-spline base of any rad. We will now formulate the basic properties according to the interpretation of books [9] and [10]. B-spline base series . and for, the derivative of the basis function is equal to zero . Z  It follows from the regulation that the derivation of B-spline basis functions of series  can be kernel as a linear combination of B-spline basis functions of the series  spline generated by B-spline base is differentiable into row . Note. Other properties of B-spline basis and B-spline basis functions we can read in or in. We obtain the regression function of a spline by a linear combination of its basis functions spline with a B-spline of the series  with nodal points Face Figure 3.8 we can see the B-spline basis functions of the series with styles evenly spaced (internally) nodes on the interval [0,10]. We will now summarize in the next section 3.5 some of the most commonly used position selection approaches and the number of nodes.  B-spline basis functions of the series  with styles evenly spaced non-internal nodes, the position of which is represented by a vertical intermittent Select location and number of nodes In this section, according to the interpretation of books [11] and [13], we will describe some of the basic position selection approaches and number of nodes. By choosing the right position and number nodes, we can achieve a significant improvement in data translation by estimating the regression function. One of the intuitive approaches to selecting the position of nodes is to select a larger number nodes in places where the regression function is more variable, while in the where this function is more stable, it is advisable to select fewer nodes. In practice, however, we soon encounter the option of evenly spreading nodes. In case, if the values of the independent variable are distributed in the interval [min (), max ()] with (approximately) the same distance, a suitable approach to node position selection is evenly spaced at a given interval. To select (internal) nodes we can use the following formula Conversely, if the values of the independent variable are not (approximately) equally spaced on the translation interval, the appropriate approach of node selection is evenly values of an independent variable. This approach guarantees us a larger number of nodes in places of the interval [min (), max ()], where we have a larger number of observations, while in parts of the interval with fewer observations, the number of nodes will be small. How we can read in [12, . 23], in contrast to the position of the nodes is for Improving the estimation of the regression function by a more important criterion of correct choice number of nodes. Among the most used approaches to assess the optimal number nodes (with a predetermined choice of their position) include e.g. decision-making based on R2adj, PRESS statistics (section 2.5.3) or e.g. in the regression model or by means of an F-test, by which we can investigate the possible validity ’of the submodel with one or more nodes removed. In the next section, we define a submodel of a regression cubic spline, namely the natural spline model. Next, we derive before the basis functions suitable for his representation Natural cubic spline As we showed in section 3.4, regression splines can be understood as parts connect polynomial functions. As we can read in [16], regression estimates The functions of the regression spline model (3.10) have in the boundary parts of the interval values of the independent variable large variance, which can lead to very inaccurate ex- trapping, the estimation of the mean values of the dependent variable outside the laying [min (), max ()]. We can potentially solve this problem by introducing daily to the regression coefficients of the regression spline model with nodes so that the regression function of the given model is in tails,  to the left of the smallest node and to the right of the largest node linear. Considering such restrictions, we come to the definition of the model of nature. spline. Note. In this section we will consider only the model of the cubic regression spline, and therefore the word cubic 'will be omitted'. Let us denote the regression function of the regression spline with nodes by f, and we need to read the regression coefficients after further work with the given function. Regressive let consider the function in the form where are regression coefficients. Regression linearity function (3.15) is guaranteed in its tails by setting Considering the given constraint, we get to the regression restrictions. coefficients that guarantee the linearity in the tails and their application we determine the prescription of the basis functions of the natural spline. For the values it is obvious that the expression  3+ will be zero for  thus the linearity of f is obtained by laying Again, similar to the regression spline model (3.10), we can consider  in the model (3.26) by means of F-test (theorem 6) and hypothesis  test the validity ’of the model of the natural model spline (3.26), namely the regression line model. Estimates of regression coefficients of the natural spline model (with nodes points) can be calculated in the software R (via the function lm ()) using ns () function, which generates a (regression) matrix with B-spline basis functions natural spline model. Note. It is important to note that, unlike the bs () function, the argument boundary. knots of the ns () function specify the nodes from which the regression function is native spline linear, nodes . Another possible way to obtain an estimate of the regression function of a natural spline is introduction of linear constraints (see section 2.4) on the vector of regression coefficients tov  in a regression spline model with a regression function in the form (3.15). Let consider linear constraints on the basis of restrictions (3.16), (3.19) and (3.20) in the form Vector of estimates of regression coefficients , which satisfies the condition T  calculate according to formula (2.21) in section 2.4. Estimation of the natural regression function the spline is uniquely given by the vector . Note. In the enclosed file, we can find the source code where we illustrate consistency of estimates of regression functions (with fixed three internal nodes), calculated by three different approaches, namely: By calculating the regression coefficients of the model (3.26) by the ps () function programmed by us, which generates a regression matrix given basic functions. By calculating regression coefficient estimates using the ns () function. By calculating the regression coefficients under the validity of the linear in the form. In this chapter we have introduced the regression spline model and its del - model of natural spline. We will now examine the difference between the estimates regression functions of given models under the validity of the larger model the model regression spline. A similar problem is solved in the article [17], where the author of the research the difference between the estimates of the regression functions of the regression model network and its submodel for the validity of the larger model Comparison of the accuracy of the regression in the model of regression and natural now As we mentioned at the beginning of the previous section 3.6, estimates of regression functions in the regression spline model (3.10) are variable in the tails, leading to an inaccurate prediction of an individual estimate of the mean value of an at the edges and outside the range of measured values of the independent variable. This we can potentially solve the problem by considering a model of natural spline (3.26), as we will show in this section, by introducing a restriction of linearity of the regression function in the regression spline model in the boundary interval , we generally obtain estimates of the mean value variable at the new point with potentially less variance. Note. The word cubic will be deleted in the following text when from the section 3.6 it is clear that the natural spline model is obtained directly from the cubic model regression spline. Consider the validity of the gres regression spline model  with  (internal nodal points and regression matrix generated basic functions. Nevertheless, let us assume the validity ’ an incorrect model, namely the natural spline model. We get this by applying linear constraints as we demonstrated in section 3.6. Let us denote vector of estimated regression coefficients of the natural spline model. By formula (2.22) applies We also know that  is the vector of estimated regression coefficients of the regression spline model. To estimate the mean value of the dependent variable at the point in the model of the natural spline, which we call , and its variance holds Inequality (3.31) implies that the variance of the estimate of the mean value depends on the nominal, the estimate of the regression function at the point  will not be natural in the model spline (as opposed to the variance of the estimate in the regression spline model) is never larger. Conversely, as shown below, the bias of the estimate  is potentially unambiguous. where the non-zero value of the mean value of the dependent variable in the model regression spline without a linear constraint on the regression coefficients directly not from definition 5. To deviate the estimate ∗ from the actual value of the regression function As we have shown, by considering the "wrong" model of natural splines During the validity of the larger model of regression splines, we obtained an estimate of the mean notes of a dependent variable that is potentially skewed, but at no point compared to the erroneous estimate of the mean value in the regression spline model v scatter. The characteristics on the basis of which we will compare the pres accuracy ’of these is called the mean square error and is defined by the following definition. Definition 11. To estimate θ the scalar parameter θ we define the mean square estimation error as Let compare the estimates of the mean value of the dependent variable of our models by the above-defined mean square error. Let us examine the conditions under which the mean value of an independent variable is estimated on the basis of the natural spline model better than on the basis of the regression model splines. From inequality (3.38) we can observe that the estimate of the mean value depending on the the natural model of natural spline will be better with a larger decrease in the scatter estimate as in the square of its deviation. One could examine what type of data a given inequality is (certainly) met, but this problem We will not deal with lematics at work. Instead, we take the results from Article [17], which states that for the increasing standard deviation of the error components and the decreasing number of observations will have an MSE estimate of the mean The value of the dependent variable in the submodel tends to be lower than the MSE estimate mean value in the larger model. We will now illustrate this theory examples. Consider the validity of a cubic spline model with three fixed nodes evenly distributed over the range of values of the independent variable. Using The generateSpline () function randomly generates a vector of regression coefficients. which determine the actual form of the regression function, and then we note the maximum number of values of independent variables we consider. We assume that the values of the independent variable are evenly distributed on the inter- value [0.10]. We generate two sets of observations. The first with the number of observations standard deviation of the error component and the second with the number of observations and the standard deviation of the error component. Thanks to the knowledge We can easily calculate the values of the exact prescription of the real regression function. MSE estimates the mean of the dependent variable in natural and regression spline models. In Figure 3.7 we see in the left MSE graph an estimate of the mean value of the dependent variable in the regression spline model (blue line) and the mean value estimate in the model of the natural spline (red line) of the first dataset, in the number observed and on the right graph possibly MSE estimates of the second dataset with the number of observations. Similar results we get by changing the seed value, by generating other regression functions, as used by us in this sample. From Figure 3.7 we can see the results are in line with the theory described above. FIG. 3.9: Graph of the MSE course estimate of the regression function of the regression spline model (RS - blue) and MSE estimation of the natural spline model regression function (PS - red line) under the validity of the regression (cubic) spline model with three fixed nodes evenly spaced on the interval [0,10]. Monte Carlo simulations To assess which type of data2 the natural spline model is better at as a model of regression splines with the fact that we considered the validity of the model of regression spline with a fixed three nodal points, we are in the previous section used the mean square error of the estimate (regression function). However, it for her calculation necessary knowledge of a real regression function, which we do not know in practice. In practice thus to evaluate which of the models3 describes the dependence between the middle value-dependent and value-independent variables are better, we use, among other things e.g. R2adj, PRESS statistics, or F-test (see sections 2.4 and 2.5.3). In this part of the work we will want to use simple Monte Carlo networks. to assess the extent to which we are able to base these characteristics on choose the right better model. We will be a model that is really better assessed by MSE estimation of the regression function of the given models. We will consider different numbers of observations and different values. standard deviation of the error component 6 vector of values of the independent variable length nj and nj vector of the values of the actual regression function in points njx, evenly distributed at intervals [0.10]. For each value 6 it generates a vector value of the dependent variable nj of length as The regression function we generated with generateSpline () is fixed and for different combinations nj and it does not change. The individual simulations differ only in realized values of the error component4. We will consider as a really better model the one whose MSE values calculated in the primary values of individual vectors are in all values of the given vector are smaller than in the second model. If none of the models did not have the lower MSE in all values of the vector, for really better We will consider the model to be the one whose average MSE is lower. Adjusted determination coefficient Consider the number of simulations. For each simulation, data of different com- bination , we estimate the regression model and native spline. For both estimates, we find the value of the adjusted coefficient determination (see section 2.5.3). Let us be, where represents the value R2adj of the regression spline model estimate of the network. We define similarly for estimating the natural spline model. We define the function by a rule Then it holds that Z1, ..., ZS is a convenient choice from the alternative distribution Alt ), where  denotes the probability that the regression spline model estimate is worse than the natural spline model estimate based on R2adj. Non- The lateral estimate of the parameter  is the relative frequency In Table below we can calculate the values of the relative frequency  for different combinations nj and . The blue color indicates the values in which the the MSE estimate of the regression spline regression function is less than the mean MSE estimation of natural spline regression function. It is rough blue marked value, where the MSE value is smaller at each point of the estimated re- aggregate function of the regression spline in which the MSE was evaluated. Similar the relationship applies to the red color and the regression function estimate of the natural spline. PRESS statistics Another characteristic of the model, on the basis of which we can decide which of models describes the relationship between the mean of an independent variable and the values The independent variable is PRESS statistics, described in more detail in section 2.5.3. We obtain the table of relative numbers in a manner analogous to that described in the previous part with the fact that the table will present the relative numbers, in which was chosen as a better model based on PRESS statistics again model natural spline. Due to the subsequent interpretation of the results, the probability Based on PRESS statistics, we select a model of natural spline, let call it q. The color difference of the values found in the table is identical to the table F-test The last approach to deciding which of the regression function estimates considered model and submodel describes a better relationship between dependent and independent variable, is through the test statistics defined in sentence 6. When the natural spline model is a submodel of the regression spline model, it is possible by means of an F-test a decision to reject or non-refusal of validity part of the natural spline. Consider a normal level of significance. The symbol ps, si denote the -value of the  simulation of the mentioned test. As in the previous In the remaining two parts, the random variables form again, a random selection from the alternative distribution, where r represents the probability that we do not reject the null hypothesis on the basis of the F-test on the validity of the submodel (at the specified value of the significance level α). we estimate the similarity ’r again’ with the relative number . In the table below we can see the relate relative frequency ’r for different combinations nj a. The error of the Czech estimate can be made as in the previous parts of this section is limited from above to 0.01 (for the number of ). Color separation the values found in the table are again identical to table As we can see in Tables, the relative numbers have different options  tend to descend towards the right the lower corner of the table to the place where the number of observations decreases a the standard deviation of the error component  increases. The form of a random variable was elected in the individual parts of this section so that the individual the similarities of , q and r corresponded to the probabilities of the regression function estimate model of natural spline is (based on R2adj, PRESS statistics and -values F-test with level better than estimating the regression function of the regression model spline. The values in the given tables are sometimes in contrast to the fact that whereas sometimes, according to the values of relative numbers, we should decide for model, which is actually based on the average MSE or MSE value below in does not apply to each evaluated point. From these values we can therefore observe these indicators are not always suitable for choosing the "right" model. You need to take into account that the results we obtain can only be judged in terms of quality, as we considered only one fixed regression function with fixed nodes, but by changing the value of the seed by generating another regression function, the results were very similar. We could possibly generalize our results by considering various regression functions. In this bachelor thesis we dealt with parametric regression models splines. At the beginning of the work, we defined the basic concepts of regression analysis. later used in Chapter 3. In this chapter, we then discussed the basic properties of various regression functions of parametric models with a focus on regression spline model. We imagined two bases for the national team regression splines, namely the truncated power base and the so-called B-spline base. Before- we built a model of natural splines and its use from the world of possible improving the accuracy of the regression function estimate. Subsequently, we deduced separately the shape of the basis functions of the given model. We then discussed the basic approaches position options and the number of nodes. In Section 3.7, we compared the accuracy of the estimates of regression functions. of the regression and natural spline model. We derived the formula of the mid- estimation errors and subsequently we qualitatively discussed the circumstances under which it is appropriate to use the natural spline model and we have illustrated this theory on simple example. In the last part of the work, we tried to use Monte Carlo simulation to find out indicators, which are commonly used in practice for model selection, can prove correctly decided in favor of a truly valid model. Based on what we have gained The results, which are captured in the tables, we came to the conclusion that these indicators users are not always able to capture the true truth